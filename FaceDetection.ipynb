{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea6940d6-0309-42a5-b538-91e8719d6c0a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Face detection demo\n",
    "This notebook contains development of face detection and age determination including the live acquisition demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87c2e32-58ac-4417-908f-5b6cb5834651",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d5f7ab-1c16-44e4-9b9a-0c55c0b3a9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import ipywidgets as ipw\n",
    "import asyncio\n",
    "from deepface import DeepFace\n",
    "from PIL import Image\n",
    "import io\n",
    "import pprint\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e3163-eda3-41fb-a737-1d40676c1efb",
   "metadata": {},
   "source": [
    "## Main acquisition and proccessing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9e4bcd2-4265-4ff8-b905-1dde26581c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main part of the code in a class\n",
    "class MainClass:\n",
    "    '''MainClass takes care of image acquisition, display and proccessing. \n",
    "    Additionally, it writes the status info into a textbox widget. \n",
    "    Acquisition is started by calling runAcquisition and stopped by calling\n",
    "    stopAcquisition. It quits gracefully.'''\n",
    "    def __init__(self):\n",
    "        # Properties storing the data needed by the class\n",
    "        self.cap = None\n",
    "        self.stopRequested = Falsen\n",
    "        self.analysisRequested = False\n",
    "        self.image = None\n",
    "        self.text = None\n",
    "        # Parameters of the output encoded image\n",
    "        self.encode_parameters = [int(cv2.IMWRITE_JPEG_QUALITY),75]\n",
    "        # Which cascade to use\n",
    "        self.xmlpath = './haarcascade_frontalface_default.xml'\n",
    "        self.AGE_BUCKETS = [\"(0-2)\", \"(4-6)\", \"(8-12)\", \"(15-20)\", \"(25-32)\", \"(38-43)\", \"(48-53)\", \"(60-100)\"]\n",
    "        \n",
    "        # Age detection using CV2\n",
    "        prototxtPath = os.path.sep.join([\"./models/deploy_age.prototxt\"])\n",
    "        weightsPath = os.path.sep.join([\"./models/age_net.caffemodel\"])\n",
    "        self.ageNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
    "        \n",
    "    async def run(self):\n",
    "        '''Main routine which acquires image, proccessess it and displays it.'''\n",
    "        # First, create the video capture object\n",
    "        self.cap = cv2.VideoCapture('/dev/video0')\n",
    "        \n",
    "        i = 0\n",
    "        while(True):\n",
    "            i = i + 1\n",
    "            # Acquire a new frame\n",
    "            ret, frame = self.cap.read()\n",
    "            \n",
    "            # Proccess the image\n",
    "            #self.detectAndAnalyzeDF(frame);\n",
    "            img = self.detectFaces(frame);\n",
    "            \n",
    "            # Display the frame in a widget\n",
    "            self.image.value = cv2.imencode('.jpg', img, self.encode_parameters)[1].tobytes()\n",
    "            \n",
    "            # Update status box\n",
    "            #self.text.value = \"Image {:0>5d} acquired!\\n\".format(i)\n",
    "            \n",
    "            # Wait for 0 time to release control to the main loop\n",
    "            await asyncio.sleep(0)\n",
    "            \n",
    "            # Break the infinite loop if stop is requested\n",
    "            if self.stopRequested:\n",
    "                break\n",
    "                \n",
    "            if self.analysisRequested:\n",
    "                self.detectAndAnalyzeDF(frame)\n",
    "        \n",
    "        # At the end, release the capture object\n",
    "        self.cap.release()\n",
    "        struct\n",
    "        \n",
    "    def runAcquisition(self):\n",
    "        # Reset stop request\n",
    "        self.stopRequested = False\n",
    "        # Create a task and use it to run main routine async\n",
    "        asyncio.create_task(self.run())\n",
    "\n",
    "    def stopAcquisition(self):\n",
    "        # Request a stop politely\n",
    "        self.stopRequested = True\n",
    "        \n",
    "    def detectFaces(self, img):\n",
    "        face_cascade = cv2.CascadeClassifier(self.xmlpath)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 6)\n",
    "        for (x, y, w, h) in faces:\n",
    "            # This is detection using on OpenCV dnn module\n",
    "            face = img[x:x+w,y:y+h]\n",
    "            faceBlob = cv2.dnn.blobFromImage(face,1.0, (227, 227),(78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "            self.ageNet.setInput(faceBlob)\n",
    "            preds = self.ageNet.forward()\n",
    "            i = preds[0].argmax()\n",
    "            age = self.AGE_BUCKETS[i]\n",
    "            ageConfidence = preds[0][i]\n",
    "    \n",
    "            cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            lbly = y - 10 if y - 10 > 10 else y + 10\n",
    "            text = \"{}: {:.2f}%\".format(age, ageConfidence * 100)\n",
    "            cv2.putText(img, text, (x, lbly),cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "        return img\n",
    "        \n",
    "    def requestAnalysis(self):\n",
    "        self.analysisRequested = True\n",
    "        \n",
    "    def detectAndAnalyzeDF(self,frame):\n",
    "        img = np.asarray(frame)\n",
    "        try:\n",
    "            obj = DeepFace.analyze(img, actions = ['age', 'gender', 'race', 'emotion'])\n",
    "            self.text.value = pprint.pformat(obj)\n",
    "            self.analysisRequested = False\n",
    "        except:\n",
    "            self.text.value = \"Face was not detected, move slightly!\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce7d971-e721-4477-900e-8e86b50df440",
   "metadata": {},
   "source": [
    "## GUI and control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48260022-8a46-4a53-b531-bb55f5af5a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c771ec0e0044672abdc97d6f288ed76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a simple GUI view\n",
    "file = open(\"initial.jpg\", \"rb\")\n",
    "image = file.read()\n",
    "file.close()\n",
    "imgbox = ipw.Image(value=image,format='jpg', width='50%')\n",
    "output = ipw.Textarea(layout=ipw.Layout(width='98%',height='300px'), disabled=True)\n",
    "startBtn = ipw.Button(description='Start')\n",
    "stopBtn = ipw.Button(description='Stop', disabled=True)\n",
    "analyzeBtn = ipw.Button(description='Analyze', disabled=True)\n",
    "\n",
    "# Create an instance of the main class\n",
    "acquisition = MainClass()\n",
    "# Assign output fields in the view\n",
    "acquisition.image = imgbox\n",
    "acquisition.text = output\n",
    "    \n",
    "# Callbacks definition\n",
    "def on_startBtn_click(btn):\n",
    "    startBtn.disabled = True\n",
    "    stopBtn.disabled = False\n",
    "    analyzeBtn.disabled = False\n",
    "    acquisition.runAcquisition()\n",
    "\n",
    "def on_stopBtn_click(btn):\n",
    "    stopBtn.disabled = True\n",
    "    analyzeBtn.disabled = True\n",
    "    startBtn.disabled = False\n",
    "    acquisition.stopAcquisition()\n",
    "    \n",
    "def on_analyzeBtn_click(btn):\n",
    "    acquisition.requestAnalysis()\n",
    "\n",
    "# Assign callbacks to buttons\n",
    "startBtn.on_click(on_startBtn_click)\n",
    "stopBtn.on_click(on_stopBtn_click)\n",
    "analyzeBtn.on_click(on_analyzeBtn_click)\n",
    "\n",
    "# Build and display the GUI window\n",
    "buttons = ipw.Box([startBtn,stopBtn,analyzeBtn], layout=ipw.Layout(display='flex',\n",
    "                                                        flex_flow='row',\n",
    "                                                        justify_content='center',\n",
    "                                                        align_items='center',\n",
    "                                                        width='100%',\n",
    "                                                        height='100px'))\n",
    "controls = ipw.VBox([buttons,output], layout=ipw.Layout(width='50%',\n",
    "                                                        justify_content='flex-end'))\n",
    "window = ipw.HBox([imgbox, controls], layout=ipw.Layout(border='solid thin', \n",
    "                                                        width='100%', \n",
    "                                                        height='auto'))\n",
    "# Display the GUI\n",
    "display(window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48994c2d-b150-4c0b-a805-d5454a2669c3",
   "metadata": {},
   "source": [
    "# Offline image age determination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c6c9a5-b209-4054-871c-e3cad73d8dd1",
   "metadata": {},
   "source": [
    "## Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44e06048-ac20-49ff-8935-33248f57ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import ipywidgets as ipw\n",
    "import asyncio\n",
    "from deepface import DeepFace\n",
    "from PIL import Image\n",
    "import io\n",
    "import pprint\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb769d-ba47-4d85-b8d3-cce3af008737",
   "metadata": {},
   "source": [
    "## DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f90a953-6ebf-4e9a-a803-4118cde07467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'age': 42,\n",
      " 'dominant_emotion': 'happy',\n",
      " 'dominant_race': 'white',\n",
      " 'emotion': {'angry': 39.19900526988569,\n",
      "             'disgust': 0.0002931245839321723,\n",
      "             'fear': 0.5934341586749743,\n",
      "             'happy': 43.17440905265916,\n",
      "             'neutral': 10.586657892067182,\n",
      "             'sad': 0.7231420768072697,\n",
      "             'surprise': 5.72306842780929},\n",
      " 'gender': 'Man',\n",
      " 'race': {'asian': 0.5643481388688087,\n",
      "          'black': 0.14442461542785168,\n",
      "          'indian': 3.125517815351486,\n",
      "          'latino hispanic': 12.56120651960373,\n",
      "          'middle eastern': 29.247862100601196,\n",
      "          'white': 54.356640577316284},\n",
      " 'region': {'h': 81, 'w': 81, 'x': 145, 'y': 62}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "obj = DeepFace.analyze('Image-1.png', actions = ['age', 'gender', 'race', 'emotion'])\n",
    "print()\n",
    "pprint.pprint(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7bab9c-9471-4e3b-8fe0-3cf3ffd10f05",
   "metadata": {},
   "source": [
    "## OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1e69990-3e5c-4d03-b732-70c99930b7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] (60-100): 99.32%\n"
     ]
    }
   ],
   "source": [
    "AGE_BUCKETS = [\"(0-2)\", \"(4-6)\", \"(8-12)\", \"(15-20)\", \"(25-32)\", \"(38-43)\", \"(48-53)\", \"(60-100)\"]\n",
    "\n",
    "prototxtPath = os.path.sep.join([\"./models/deploy_age.prototxt\"])\n",
    "weightsPath = os.path.sep.join([\"./models/age_net.caffemodel\"])\n",
    "ageNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
    "\n",
    "# load an image\n",
    "face = cv2.imread(\"Image-1.png\")\n",
    "faceBlob = cv2.dnn.blobFromImage(face,1.0, (227, 227),(78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "# make predictions on the age and find the age bucket with\n",
    "# the largest corresponding probability\n",
    "ageNet.setInput(faceBlob)\n",
    "preds = ageNet.forward()\n",
    "i = preds[0].argmax()\n",
    "age = AGE_BUCKETS[i]\n",
    "ageConfidence = preds[0][i]\n",
    "# display the predicted age to our terminal\n",
    "text = \"{}: {:.2f}%\".format(age, ageConfidence * 100)\n",
    "print(\"[INFO] {}\".format(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3-venv",
   "language": "python",
   "name": "python3-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
